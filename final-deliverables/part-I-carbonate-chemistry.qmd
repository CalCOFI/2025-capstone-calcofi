---
title: "Carbonate Chemistry Trend Analysis"
format:
  html:
    code-fold: true

execute: 
  warning: false
  message: false
---


# Overview

This project is sponsored by California Cooperative Oceanic Fisheries Investigations (CalCOFI), an organization founded in 1949 to study the ecological aspects of the Pacific sardine collapse off of the coast of California. CalCOFI is committed to studying California’s coastal marine environment and collecting relevant oceanographic data in order to provide insight on important climate change related topics such as renewable energy, integrated ocean management, and marine spatial planning.

We aim to extend the research done in “A 37 year record of ocean acidification in the Southern California current” by Wolfe et al. on the yearly rate of change of certain carbonate chemistry variables by examining all available carbonate chemistry data collected across CalCOFI observation stations rather than only surface data collected at station 90.90.

In particular, it is of interest to examine how the measurements of important ocean carbon chemistry and oceanographic variables have changed over time, namely total alkalinity (TA), total dissolved inorganic carbon (DIC), the Revelle Factor, pH, pCO2, Omega aragonite ($\Omega_{aragonite}$), Omega calcite ($\Omega_{calcite}$), temperature, salinity, and CO2-. Additionally, we aim to examine whether there is a difference in carbon uptake between coastal and non coastal stations due to coastal upwelling. Finally, we wish to assess the performance of Empirical Seawater Property Estimation Routines (ESPER) in predicting carbonate chemistry variables across different depths with easy to collect oceanographic variables such as temperature and salinity as inputs. 

# Data

CalCOFI samples from a predetermined sampling grid off the coast of California on a quarterly basis. Typical stations are set 40 nautical miles apart. At each sampling point, identified by a station and line number, CalCOFI lowers a carousel of 24 bottles into the water, which collect seawater samples from around 20 different depths (typically ranging from 20 to 515 meters). Researchers on the ship then measure oceanographic values such as the temperature, salinity, macronutrient concentration and other properties of these samples. This results in the oceanographic dataset used in this study. 

While oceanographic data is measured on every CalCOFI cruise, carbonate chemistry values such as TA and DIC are only occasionally measured from the collected water samples and are stored in their own dataset. 

The oceanographic dataset can be found here https://calcofi.org/data/oceanographic-data/bottle-database/, while the carbonate chemistry dataset can be found here https://calcofi.org/data/oceanographic-data/dic/

The above datasets must be downloaded manually and put in the data folder in order for this qmd to run. They are unfortunately too large to upload to Github.

In order to proceed, we need to merge these two datasets into one, the process of which can be seen below. 

```{r}
library(tidyverse)

# Read in oceanographic bottle data
hydro_bottle <- read_csv(
  "part-1/data/194903-202105_Bottle.csv",
   # change encoding
   locale=locale(encoding="latin1"),
   # increase guess_max to correctly guess column types
   guess_max = Inf
)

# Read in cast data
cast_bottle <- read_csv("part-1/data/194903-202105_Cast.csv")

# Read in carbonate chemistry bottle data
cc_bottle <- read_csv("part-1/data/carbonate_chem_bottle.csv")

# Drop first row (containing units) of carbonate chemistry bottle data
cc_bottle <- cc_bottle[2:nrow(cc_bottle),]

# Merge oceanographic and cast data based on Cst_Cnt (Cast Count) and Sta_ID (Station ID)
hydro_bottle <- hydro_bottle %>%
  left_join(
    cast_bottle,
    by = join_by(Cst_Cnt, Sta_ID)
  )

# Prepare hydro bottle data for merging
depth_tol <- 0.9
hydro_bottle <- hydro_bottle %>%
  mutate(
    Date = as.Date(Date, format = c("%m/%d/%Y"))
  ) %>%
  mutate(
    Year = year(Date),
    Month = month(Date)
  ) %>%
  mutate(
    Depthm_Upper = Depthm + depth_tol,
    Depthm_Lower = Depthm - depth_tol
  )

# Prepare carbonate chemistry data for merging
cc_bottle <- cc_bottle %>%
  # Create new date column for merging
  mutate(
    Date = as.Date(
      paste(Month_UTC, Day_UTC, Year_UTC, sep = "/"),
      tryFormats = c("%m/%d/%Y")
    ),
    .before = Year_UTC
  ) %>%
  # Change column types for merging
  mutate(
    Depth = as.double(Depth),
    Latitude = as.double(Latitude),
    Longitude = as.double(Longitude)
  )

# Merge carbonate chemistry and oceanographic bottle data based on date, location, and depth
merged_bottle_data <- inner_join(
  cc_bottle, 
  hydro_bottle,
  by = join_by(Month_UTC == Month,
               Year_UTC == Year,
               Station_ID == Sta_ID,
               between(Depth, Depthm_Lower, Depthm_Upper)),
  suffix = c(".cc", ".hydro")
)

# Save merged data
write_csv(merged_bottle_data, "part-1/data/merged_bottle_data.csv")
```

## EDA

### Sampling Over Time

The below histogram shows the number of observations recorded each year. Note that there is a gap from 2002 to 2007.

```{r}
merged_bottle_data |> 
  ggplot() +
  geom_histogram(aes(x = Date.cc), fill = 'blue', color = "black") + 
  theme_bw() +
  xlab("Year") + 
  ylab("Observations") + 
  labs(title = "Number of Observations Per Year")
```

### Map of Sampling Stations

Next we'll take a closer look at what our data looks like by creating a map of the sampling stations.

```{r}
library(gt)
library(sf)
library(rnaturalearth)
library(scales)

world <- ne_countries(scale = "medium", returnclass = "sf")

stations <- merged_bottle_data %>%
  group_by(
    Station_ID
  ) %>%
  summarize(
    lat = mean(Latitude),
    lon = mean(Longitude),
    observations = n()
  )

ggplot(
  data = world
) +
  geom_sf(fill = "antiquewhite1") +
  geom_point(
    data = stations,
    aes(
      x = lon,
      y = lat,
      fill = observations,
    ),
    color = "black",
    pch = 21,
    show.legend=TRUE # force shape to always show in legend
  ) +
  coord_sf(
    xlim = c(stations$lon %>% min() - 2, stations$lon %>% max() + 2),
    ylim = c(stations$lat %>% min(), stations$lat %>% max())
  ) +
  # create color scale for slope estimates
  scale_fill_gradient2(
    low = "#d7191c",
    high = "#2c7bb6",
    mid = "#ffffbf"
  ) +
  # create custom shape scale
  scale_shape_manual(
    values = c("Yes" = 24, "No" = 21),
    drop = FALSE # force both shapes to always show in legend
  ) +
  theme(
    panel.grid.major = element_line(
      color = gray(0.5), 
      linetype = "solid", 
      linewidth = 0.5
    ), 
    panel.background = element_rect(fill = "aliceblue")
  ) +
  # fix the order of the legends
  guides(
    fill = guide_colorbar(order = 1),
    size = guide_legend(order = 50),
    shape = guide_legend(order = 98)
  ) +
  labs(
    x = NULL,
    y = NULL,
    title = "Map of CalCOFI Sampling Stations",
    fill = str_wrap("Number of Observations", 40)
  )

```

### Outliers

The below histogram shows the distribution of salinity in the dataset. Note that there is a small group of observations with salinity values in the 20's. These are unreasonable values that are likely a result of a data entry error, so we will be mindful to remove these observations from future analysis.

```{r}
merged_bottle_data |> 
  ggplot() +
  geom_histogram(aes(x = Salnty), fill = 'blue', color = "black") + 
  theme_bw() +
  xlab("Salinity") + 
  ylab("Observations") + 
  labs(title = "Salinity Distribution")
```


## CO2SYS

In the merged dataset above, the only carbonate chemistry variables that have been recorded are TA and DIC. To remedy this, we can use CO2SYS, a program that uses a mechanistic model to calculate all other carbonate chemistry variables given at least two carbonate chemistry variables as well as envirornmental variables such as
temperature and salinity. The CO2SYS routines are written and contained in MATLAB files, so we unfortunately cannot run them in this document. However, the folder titled CO2SYS contains both the CO2SYS routines as well as a script called CO2SYS_calc.m where we apply CO2SYS to our data in order to get the other carbonate variables of interest. 

The resulting dataset of CO2SYS calculated varaibles is named CO2SYS_out.csv in the data folder.


# ESPER Model Validation



# Carbon Trend Analysis

As mentioned prior, we are interested in examining how carbonate chemistry variables have been changing over time, and whether this temporal trend differs among coastal versus non coastal stations. In particular, we are interested in the following variables:
- `TA`
- `DIC`
- `pCO2in`
- `pHin`
- `CO3in`
- `OmegaCAin`
- `OmegaARin`

As pointed out in  “Advancing best practices for assessing trends of ocean acidification time series” by Sutton et al, oceanographic carbonate time series data is likely to have a seasonal trend. Below we create a function we can apply to our data to deal with this by following the procedure that Sutton outlined in the above article.

```{r}
sea_dtd_data <- function(qty, df, date_col) {
  # Seasonally detrend in a dataframe based on Sutton, A. J. et al. (2022)
  #   qty: vector of column names of variables to be detrended
  #   df: dataframe with observations to be detrended
  #   date_col: date column to be used for fitting
  # Outputs original dataframe with appended columns of detrended data with name format qty_dtd
  
  # Check if date_col is in decimal format and convert if not
  if (!is.double(date_col)) {
    df <- df %>%
      mutate(
        Date_Dec = decimal_date(get(date_col))
      )
  }
  
  # Detrend data for each desired quantity
  for (i in qty) {
    # Extract overall linear trend
    lin_trend <- lm(get(i) ~ Date_Dec, data = df, na.action = na.exclude)
    
    # Remove overall linear trend
    df <- df %>%
      mutate(
        lin_dtd_obs = as.vector(residuals(lin_trend))
      )
    
    # Bin observations into a three month sliding scale
    df_minus <- df %>% 
      mutate(
        bin_month = ifelse((Month_UTC - 1) == 0, 12, Month_UTC - 1)
      )
    df_0 <- df %>%
      mutate(
        bin_month = Month_UTC
      )
    df_plus <- df %>%
      mutate(
        bin_month = ifelse((Month_UTC + 1) == 13, 1, Month_UTC + 1)
      )
    df_binned <- bind_rows(df_minus, df_0, df_plus)
    
    # Compute monthly means of each 3-month bin
    monthly_means_df <- df_binned %>%
      group_by(
        bin_month
      ) %>%
      summarize(
        monthly_mean = mean(lin_dtd_obs, na.rm = TRUE),
        .groups = "drop"
      )
    
    # Compute anomaly for each bin
    monthly_means_df <- monthly_means_df %>%
      mutate(
        anomaly = monthly_mean - mean(monthly_mean, na.rm = TRUE)
      )
    
    # Subtract anomalies from observations
    df <- df %>% 
      left_join(
        monthly_means_df,
        by = join_by(Month_UTC == bin_month)
      ) %>%
      mutate(
        i_dtd = get(i) - anomaly
      ) %>%
      rename_with(
        .cols = i_dtd,
        ~ paste0(i, "_dtd")
      )
    
    # Remove extra columns that are no longer needed
    df <- df %>% 
      select(
        -c(lin_dtd_obs,monthly_mean,anomaly)
      )
  }
  
  # Return dataframe as output
  return(df)
}
```


We take two approaches in achieving this, namely modeling each of the variables of interest at each station individually, and creating a hierarchical model that looks at all of the data at once. 

Since the relationship between depth and different carbonate chemistry variables is hard to model, especially since each variable has a different relationship with depth, we have decided to limit our study to only observations with depths of less than 20 m. 

## Station by Station Models

Our first approach involves fitting linear regression models for each station and carbonate chemistry variable regressed against time. The code for which can be seen below. 

```{r}
library(gt)
library(sf)
library(rnaturalearth)
library(scales)
library(latex2exp)
library(FDRestimation)
library(stringr)
library(ggrepel)

# READ AND PROCESS DATA ---------------------------------------------------

# Load merged bottle data and CO2SYS output
merged_bottle_data <- read_csv("part-1/data/merged_bottle_data.csv")
co2sys_out <- read_csv("part-1/data/CO2SYS_out.csv")

# Combine merged bottle data and CO2SYS output and filter out anomalies
bottle_co2sys <- bind_cols(merged_bottle_data, co2sys_out) %>%
  filter(
    Salnty > 30,
    Depth < 1000
  )

# Create vector of variables to be detrended
qty <- c("TA","DIC","pCO2in","pHin","CO3in","OmegaCAin","OmegaARin")

# Detrend variables of interest
bottle_co2sys <- sea_dtd_data(qty, bottle_co2sys, "Date.cc")

# Get the names of all CalCOFI stations in the data and their locations
stations <- bottle_co2sys %>%
  group_by(
    Station_ID
  ) %>%
  summarize(
    lat = mean(Latitude),
    lon = mean(Longitude),
    observations = n()
  )

# FIT LINEAR MODELS -------------------------------------------------------

# create fits and results objects for surface (<20m)
surf_fits <- NULL
surf_results <- NULL

# iterate through stations and fit linear models for each quantity
for (i in 1:nrow(stations)) {
  # extract data for station i
  data <- bottle_co2sys %>% filter((Station_ID == stations$Station_ID[i]) & (Depth <= 20))
  for (j in 1:length(qty)) {
    # check if all values are NA
    if (data %>% select(paste0(qty[j],"_dtd")) %>% is.na() %>% `!`() %>% sum() == 0) {
      # if so, add NA to list of fits
      surf_fits[[(i-1)*length(qty)+j]] <- NA
      # and add row of NA values to surf_results for the corresponding quantity and station
      surf_results <- bind_rows(surf_results, c(
        station = stations$Station_ID[i],
        lat = stations$lat[i],
        lon = stations$lon[i],
        qty = qty[j], 
        c(Estimate = NA, `Std. Error` = NA, `t value` = NA, `Pr(>|t|)` = NA), 
        n = NA, 
        r2 = NA))
    }
    else { # fit the linear model
      fit <- lm(as.formula(paste(paste0(qty[j],"_dtd"),"~","Date_Dec")), data = data, na.action = na.exclude)
      # add fit to list of fits
      surf_fits[[(i-1)*length(qty)+j]] <- fit
      # add coefficient estimate and regression statistics in a new row to surf_results
      surf_results <- bind_rows(surf_results, c(
        station = stations$Station_ID[i],
        lat = stations$lat[i],
        lon = stations$lon[i], 
        qty = qty[j], 
        if(nrow(coef(summary(fit))) == 1) c(Estimate = NA, `Std. Error` = NA, `t value` = NA, `Pr(>|t|)` = NA) else coef(summary(fit))[2,], 
        n = summary(fit)$df[2] + 2, 
        r2 = summary(fit)$r.squared))
    }
  }
}
```

Note that we have stored the results in a new dataframe called surf_results. For easier analysis, we are going to add a column stating whether the model had a statistically significant temporal trend, as well as a new dataframe that contains the proportion of significant models at each station. We use the proportion rather than the exact number since the presence of missing observations causes some stations to not have sufficient observations to properly fit all of the models.

```{r}
surf_results <- surf_results %>%
  # convert numeric columns to numeric vectors
  mutate(
    across(-c(station, qty), as.numeric)
  ) |> 
  filter(!is.na(`Pr(>|t|)`))

surf_results <- surf_results |> 
  mutate(adj_p_value = (p.fdr(pvalues = surf_results$`Pr(>|t|)`))$fdrs) |> 
  mutate(sigp = factor(ifelse(adj_p_value < 0.05, 1, 0), levels = c(1,0), labels = c("Yes", "No")),
         sigp_ind = ifelse(adj_p_value < 0.05, 1, 0),
         sig_label = ifelse(adj_p_value < 0.05, paste("(",station,",",signif(Estimate, digits = 4), ")"), "")) |> 
  filter(n >= 10)


sign_stations <- surf_results |> group_by(station) |> 
  summarize(min_n = min(n),
            max_n = max(n),
            mean_n = mean(n),
            lat = mean(lat),
            lon = mean(lon),
            num_sig = sum(sigp_ind),
            prop_sig = mean(sigp_ind)) |> 
  ungroup()
```


While it's harder to get a more general view of trends through station by station modeling, it does have one major advantage: maps! Below we will create maps for each variable of interest.

```{r}
# import map for plotting
world <- ne_countries(scale = "medium", returnclass = "sf")

# create vector of (full) names for each quantity
qty_names <- c("Temperature", "Salinity", "TA", "DIC", "pCO2", "Revelle Factor", "pH", "CO3$^{2-}$", "$\\Omega_{calcite}$", "$\\Omega_{aragonite}$")

# create vector of units for each quantity
units <- c("$^\\circ$C yr$^{-1}$", "yr$^{-1}$", "µmol kg$^{-1}$ yr$^{-1}$", 
           "µmol kg$^{-1}$ yr$^{-1}$", "µatm yr$^{-1}$","yr$^{-1}$", 
           "yr$^{-1}$", "µmol kg$^{-1}$ yr$^{-1}$", "yr$^{-1}$", "yr$^{-1}$")

# generate plots for surface fits
for (i in 1:10) {
  # extract data for quantity i
  data <- surf_results %>%
    filter(
      qty == qty[i]
    ) %>%
    # filter out stations with n<=15 observations used in the fit
    filter(
      (!is.na(Estimate)) & (n > 15)
    )
  
  # create plot of slope by station
  ggplot(
    data = world
  ) +
    geom_sf(fill = "antiquewhite1") +
    geom_point(
      data = data,
      aes(
        x = lon,
        y = lat,
        fill = Estimate, # estimated slope
        size = n, # number of observations
        shape = sigp # if estimate is statistically significant
      ),
      color = "black",
      show.legend=TRUE # force shape to always show in legend
    ) + 
    geom_text_repel(data = data, aes(x = lon, y = lat, label=sig_label), max.overlaps = 30, box.padding = 1) +
    # manually adjust coordinates
    coord_sf(
      xlim = c(surf_results$lon %>% min() - 2, surf_results$lon %>% max() + 2),
      ylim = c(surf_results$lat %>% min(), surf_results$lat %>% max())
    ) +
    # create color scale for slope estimates
    scale_fill_gradient2(
      low = "#d7191c",
      high = "#2c7bb6",
      mid = "#ffffbf"
    ) +
    # create custom shape scale
    scale_shape_manual(
      values = c("Yes" = 24, "No" = 21),
      drop = FALSE # force both shapes to always show in legend
    ) +
    theme(
      panel.grid.major = element_line(
        color = gray(0.5), 
        linetype = "solid", 
        linewidth = 0.5
      ), 
      panel.background = element_rect(fill = "aliceblue")
    ) +
    # fix the order of the legends
    guides(
      fill = guide_colorbar(order = 1),
      size = guide_legend(order = 50),
      shape = guide_legend(order = 98)
    ) +
    labs(
      x = NULL,
      y = NULL,
      title = TeX(paste("Estimated Slope for", qty_names[i], "by Station at Surface (Depth<=20m, N>15)", paste0("[",units[i],"]"))),
      color = "Estimate",
      size = "N",
      shape = "adjusted p<0.05",
      # caption = TeX(paste("Mean Slope (weighted by $N$):", format(round(weighted.mean(data$Estimate, data$n), 4), nsmall = 4), units[i]))
    )
  
  # save plots
  ggsave(paste0("part-1/images/surf_", qty[i], "_by_station.png"), bg = "white", units = "in", width = 8, height = 8)
}
```

All of the plots can be found in the images folder, below we can see an example plot of DIC. Note that the stations with a statistically significant temporal trends are shaped as triangles and are labeled with the station number and point estimator.


```{r, echo=FALSE}
knitr::include_graphics("part-1/images/surf_DIC_by_station.png")
```

We can also use the `sign_stations` dataframe we made earlier to create a map that shows the proportion of models with a significant temporal trend for each station. 
```{r}
ggplot(
  data = world
) +
  geom_sf(fill = "antiquewhite1") +
  geom_point(
    data = sign_stations,
    aes(
      x = lon,
      y = lat,
      fill = mean_n, # mean number of observations used for models
      size = prop_sig, # number of significany predictors
    ),
    color = "black",
    pch = 21,
    show.legend=TRUE # force shape to always show in legend
  ) +
#  geom_text(data = sign_stations, nudge_y = -.07 , size = 2, aes(x = lon, y = lat, label = station)) + 
  # manually adjust coordinates
  coord_sf(
    xlim = c(sign_stations$lon %>% min() - 2, sign_stations$lon %>% max() + 2),
    ylim = c(sign_stations$lat %>% min(), sign_stations$lat %>% max())
  ) +
  # create color scale for slope estimates
  scale_fill_gradient2(
    low = "#d7191c",
    high = "#2c7bb6",
    mid = "#ffffbf"
  ) +
  # create custom shape scale
  scale_shape_manual(
    values = c("Yes" = 24, "No" = 21),
    drop = FALSE # force both shapes to always show in legend
  ) +
  theme(
    panel.grid.major = element_line(
      color = gray(0.5), 
      linetype = "solid", 
      linewidth = 0.5
    ), 
    panel.background = element_rect(fill = "aliceblue")
  ) +
  # fix the order of the legends
  guides(
    fill = guide_colorbar(order = 1),
    size = guide_legend(order = 50),
    shape = guide_legend(order = 98)
  ) +
  labs(
    x = NULL,
    y = NULL,
    title = "Number of Time Significant Variables by Station at Surface (Depth<=20m)",
    fill = "Mean Observations per Model",
    size = str_wrap("Proportion of Variables of Interest with Significant Temporal Trend", 40)
  )
```
From this plot we see that stations further from the coast tend to have a higher proportion of variables of interest that have a significant temporal trend. We are interested in performing a hypothesis testing to see if this is truly the case, which leads us to creating hierarchical models. 

## Hierarchical Model

In order to rigorously test the overall temporal trend for each of our variables of interest, as well as to determine if their is an interaction between time and coastal stations we will use hierarchical models.

The first step is to determine which stations to classify as coastal stations. This process was done by hand using our intuition, and a map of the results can be seen below. 

```{r}
st_test <- stations
st_test$st <- 0
st_test$line <- 0
for (i in 1:nrow(st_test)){
  st_test$st[i] <- str_split(st_test$Station_ID[i], " ")[[1]][1]
  st_test$line[i] <- str_split(st_test$Station_ID[i], " ")[[1]][2]
}
st_test <- st_test |> 
mutate(coastal = case_when(
  (as.numeric(st) <= 86) & (as.numeric(line) < 70) ~ TRUE,
  (as.numeric(st) < 90) & (as.numeric(line) < 40) ~ TRUE,
  (as.numeric(st)) >= 90 & (as.numeric(line) < 35) ~ TRUE,
  TRUE ~ FALSE
))


ggplot(
  data = world
) +
  geom_sf(fill = "antiquewhite1") +
  geom_point(
    data = st_test,
    aes(
      x = lon,
      y = lat,
      color = coastal,
      size = observations
    ),
    show.legend=TRUE # force shape to always show in legend
  ) +
  # manually adjust coordinates
  coord_sf(
    xlim = c(st_test$lon %>% min() - 2, st_test$lon %>% max() + 2),
    ylim = c(st_test$lat %>% min(), st_test$lat %>% max())
  ) +
  # create custom shape scale
  scale_shape_manual(
    values = c("Yes" = 24, "No" = 21),
    drop = FALSE # force both shapes to always show in legend
  ) +
  scale_color_manual(labels = c("No", "Yes"), values = c("#F8766D", "#619CFF")) + 
  theme(
    panel.grid.major = element_line(
      color = gray(0.5), 
      linetype = "solid", 
      linewidth = 0.5
    ), 
    panel.background = element_rect(fill = "aliceblue")
  ) +
  # fix the order of the legends
  guides(
    fill = guide_colorbar(order = 1),
    size = guide_legend(order = 50),
    shape = guide_legend(order = 98)
  ) + 
  labs(
    title = "Map of Sampling Stations",
    x = "Longitude",
    y = "Latidude",
    size = "Observations",
    col = "Coastal"
  ) + 
  theme(
    plot.title = element_text(size = 28, hjust = 0.5),      # Title
    legend.title = element_text(size = 20),                 # Legend title
    legend.text = element_text(size = 18),                                  # Legend labels
    axis.title = element_text(size = 20)
  )
```


Next we need to actually fit the hierarchical models. These models will be specified as follows: 

$$
response_{ij} = time_i + coastal\_int_i + time_i*coastal\_int_i + depth_i 
$$
with the exception of pH and TA which will instead be modeled: 
$$
response_{ij} = time_i + coastal\_int_i + time_i*coastal\_int_i
$$
since these variables have a different relationship with depth, and omitting depth from the model results in a better AIC value.

Note that `coastal_int` is an indicator variable for if the observation belongs to a coastal station. 

The following code outlines the model fitting process. 
```{r}
library(tidyverse)
library(lme4)
library(gt)
library(MuMIn)
library(ModelMetrics)


merged_bottle_data <- read_csv(here::here("data/merged_bottle_data.csv"))
co2sys_out <- read_csv(here::here("data/CO2SYS_out.csv"))

# Combine merged bottle data and CO2SYS output and filter out anomalies
surf_bottle_co2sys <- bind_cols(merged_bottle_data, co2sys_out) %>%
  filter(
    Salnty > 30,
    Depth <= 20
  )

qty <- c("T_degC", "Salnty","TA","DIC","pCO2in", "RFin", "pHin","CO3in","OmegaCAin","OmegaARin")

# Detrend variables of interest
surf_bottle_co2sys <- sea_dtd_data(qty, surf_bottle_co2sys, "Date.cc")

# Average repeated measures

surf_bottle_co2sys <- surf_bottle_co2sys |> group_by(Station_ID, Depth, Date.cc) |>
  select(Station_ID, Depth, Date_Dec, TA_dtd, T_degC_dtd, DIC_dtd, pCO2in_dtd, RFin_dtd, pHin_dtd, CO3in_dtd, OmegaCAin_dtd, OmegaARin_dtd, Salnty_dtd) |> 
  summarize(Station_ID = max(Station_ID),
            Depth = max(Depth),
            Date_Dec = max(Date_Dec),
            TA_dtd = mean(TA_dtd, na.rm = T),
            DIC_dtd = mean(DIC_dtd, na.rm = T),
            T_degC_dtd = mean(T_degC_dtd, na.rm = T),
            pCO2in_dtd = mean(pCO2in_dtd, na.rm = T),
            RFin_dtd = mean(RFin_dtd),
            pHin_dtd = mean(pHin_dtd),
            CO3in_dtd = mean(CO3in_dtd, na.rm = T),
            OmegaCAin_dtd = mean(OmegaCAin_dtd, na.rm = T),
            OmegaARin_dtd = mean(OmegaARin_dtd, na.rm = T),
            Salnty_dtd = mean(Salnty_dtd, na.rm = T)) |> 
  ungroup()

surf_bottle_co2sys$st <- 0
surf_bottle_co2sys$line <- 0
for (i in 1:nrow(surf_bottle_co2sys)){
  surf_bottle_co2sys$st[i] <- str_split(surf_bottle_co2sys$Station_ID[i], " ")[[1]][1]
  surf_bottle_co2sys$line[i] <- str_split(surf_bottle_co2sys$Station_ID[i], " ")[[1]][2]
}
surf_bottle_co2sys <- surf_bottle_co2sys |> 
  mutate(coastal = case_when(
    (as.numeric(st) <= 86) & (as.numeric(line) < 70) ~ TRUE,
    (as.numeric(st) < 90) & (as.numeric(line) < 40) ~ TRUE,
    (as.numeric(st)) >= 90 & (as.numeric(line) < 35) ~ TRUE,
    TRUE ~ FALSE
  ))

# Modeling

# Depth is either included or not based on if it increases AIC 
# models for pH, and TA do not include depth

surf_bottle_co2sys <- surf_bottle_co2sys |> 
  filter(!is.na(Date_Dec)) |> 
  mutate(Date_Dec_cen = Date_Dec - min(surf_bottle_co2sys$Date_Dec))

omegaARin_mod <- lmer(
  OmegaARin_dtd ~ Date_Dec + Date_Dec:coastal + Depth + coastal + (1 | Station_ID),
  data = surf_bottle_co2sys,
  na.action = na.omit,
  REML = FALSE
)

TA_mod <- lmer(
  TA_dtd ~ Date_Dec + Date_Dec:coastal + coastal + (1 | Station_ID),
  data = surf_bottle_co2sys,
  na.action = na.omit,
  REML = FALSE
)

DIC_mod <- lmer(
  DIC_dtd ~ Date_Dec + Date_Dec:coastal +  Depth + coastal + (1 | Station_ID),
  data = surf_bottle_co2sys,
  na.action = na.omit,
  REML = FALSE
)


pCO2in_mod <- lmer(
  pCO2in_dtd ~ Date_Dec + Date_Dec:coastal + Depth + coastal + (1 | Station_ID),
  data = surf_bottle_co2sys,
  na.action = na.omit,
  REML = FALSE
)


pHin_mod <- lmer(
  pHin_dtd ~ Date_Dec + Date_Dec:coastal + coastal + (1 | Station_ID),
  data = surf_bottle_co2sys,
  na.action = na.omit,
  REML = FALSE
)

CO3in_mod <- lmer(
  CO3in_dtd ~ Date_Dec + Date_Dec:coastal +  Depth + coastal + (1 | Station_ID),
  data = surf_bottle_co2sys,
  na.action = na.omit,
  REML = FALSE
)

omegaCAin_mod <- lmer(
  OmegaCAin_dtd ~ Date_Dec + Date_Dec:coastal +  Depth + coastal + (1 | Station_ID),
  data = surf_bottle_co2sys,
  na.action = na.omit,
  REML = FALSE
)

```

Next we will format the results into a table.

```{r}
models <- list(TA_mod, DIC_mod, pCO2in_mod, pHin_mod, CO3in_mod, omegaCAin_mod, omegaARin_mod)
lapply(
  1:7,
  function(i) {
    c(qty = qty[i], 
      int_est = coef(summary(models[[i]]))[nrow(coef(summary(models[[i]]))),1], 
      t_est = coef(summary(models[[i]]))[2,1], n = nobs(models[[i]]),
      r2 = r.squaredGLMM(models[[i]])[2],
      int_CI = paste0("(", format(round(confint(models[[i]])[nrow(confint(models[[i]])),1], 5), nsmall = 5), ", ", format(round(confint(models[[i]])[nrow(confint(models[[i]])),2], 5), nsmall = 5), ")"),
      t_CI = paste0("(", format(round(confint(models[[i]])[4,1], 5), nsmall = 5), ", ", format(round(confint(models[[i]])[4,2], 5), nsmall = 5), ")"),
      t_p = coef(summary(models[[i]]))[2,3],
      int_p= coef(summary(models[[i]]))[nrow(coef(summary(models[[i]]))),2])
  }
) %>%
  # combine results into a dataframe
  bind_rows() %>%
  # convert appropriate columns to numeric
  mutate(
    across(-c(qty, t_CI, int_CI), as.numeric)
  ) %>%
  # rename quantities vector for tidier appearance in table
  mutate(
    qty = c("TA", "DIC", "*p*CO2", "pH", "CO~3~<sup>2-</sup>", "Ω~calcite~", "Ω~aragonite~")
  ) %>%
  gt(
    rowname_col = "qty"
  ) %>%
  tab_spanner(
    label = "Temporal Effect",
    columns = c(t_est, t_CI, t_p)
  ) |> 
  tab_spanner(
    label = "Temporal-Coastal Interaction Effect",
    columns = c(int_est, int_CI, int_p)
  ) |> 
  tab_header(
    title = "Carbonate Chemistry Mixed Effect Regression Statistics for CalCOFI Stations"
  ) %>%
  tab_row_group(
    label = "Seawater carbonate chemistry",
    rows = c("DIC", "TA", "*p*CO2")
  ) %>%
  tab_row_group(
    label = "Ocean acidification indicators",
    rows = c("pH", "CO~3~<sup>2-</sup>", "Ω~calcite~", "Ω~aragonite~")
  ) |> 
  # add label to row names
  tab_stubhead(
    label = "Parameter"
  ) %>%
  # rename columns
  cols_label(
    t_est = "Estimate",
    int_est = "Estimate",
    t_p = "p-value",
    int_p = "p-value",
    r2 = md("r<sup>2</sup>"),
    t_CI = "95% CI",
    int_CI = "95% CI"
    
  ) %>%
  fmt_markdown(
    columns = c(qty, t_CI, int_CI)
  ) %>%
  fmt_number(
    columns = c("t_est", "int_est", "t_p", "int_p", "r2"),
    decimals = 4
  ) %>%
  sub_small_vals(
    columns = c(t_p, int_p),
    threshold = 0.0001
  ) %>%
  opt_stylize(
    style = 3
  ) |> 
  tab_options(
    heading.title.font.size = 25
  )
```
As seen in the above table we can conclude that all variables of interest except for TA have a significant temporal effect, and all variables of interest except for pH, DIC, and pCO2 have significant temporal-coastal interactions, which implies that there is evidence for reduced carbon uptake at coastal stations due to coastal upwelling.

# Acknowledgements
A special thank you to our mentors, Erin Satterthwaite, Todd Martz, Brice Semmens, and Erika McPhillips, for their guidance and support.